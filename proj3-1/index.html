<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Vincent Lee, Kevin Xiong</h2>

<!-- Add Website URL -->

<br><br>




<div>

<h2 align="middle">Overview</h2>
<p>
    We created a render that can render realisitic images using path tracing.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->
<h3>
  Ray Generation
</h3>
<p>
	The first task was to set up the camera. Since the camera depends on the field of view, we mapped the position of the camera to the camera sensor based on the dimensions of the sensor. The formula used was <pre>cameraX = minX + ((maxX - minX)*x) </pre> and <pre>cameraY = minY + ((maxY - minY)*y)</pre> where max/minX and max/minY are the dimensions of the sensor and x, y are the coordinates of the camera in the image space. Then the camera space was generated and converted to world space using c2w. The next task was to be able to generate rays. We approached this part similarly to how we did supersampling. We traversed the grid of pixels, and we took a specified number of samples, each of which generated a ray using generate_ray from the previous task. The samples were then averaged, giving us the value of the pixel. The final part was writing the intersection methods for triangles and spheres. These let us test whether or not a ray intersects an object. For triangles, we used barycentric coordinates to test if the intersection is within the triangle, and for spheres, we used quadratic equations and solved for the roots. Aftering finding the intersection t, we fill in and update an Intersection object if t lies within the ray’s min_t and max_t.
</p>
<br>

<h3>
  Scene Intersection
</h3>
<p>
    The triangle intersection algorithm we used was the Möller Trumbore algorithm, as shown in the image below. We let P0, P1, and P2 correspond to the given vertex parameters p1, p2, and p3 respectively, and O was set to the origin of the ray. Solving for S, S1, S2, E1, and E2 allowed us to solve for t, b1, and b2. Then, b1, b2, and 1- b1 - b2 give us the barycentric coordinates, and if they are all greater than or equal to 0, then the intersection is inside the triangle.
</p>
<br>

<h3>
  Results
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBspheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres.dae with normal shading</figcaption>
      </td>
      <td>
        <img src="images/bunny_part1.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae with normal shading</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  BVH Construction Algorithm
</h3>
<p>
	To construct the bvh, we iterated through all of the primitives and calculated an average centroid between them. If the number of primitives was less than or equal to the max_leaf_size, then we would create a leaf node with all the primitives passed in and return. If not, we then came up with a huerisitc to split them into left and right nodes. Using the average centroid we calculated, and on each axis, we counted the number of primitives centroids that were less than the average on the respective axis. We then chose the axis that would split most evenly. With that we were able to create two close to even lists for the left and right nodes, one with centroid values less than average, and another one with centroid values greater than or equal to the average.

</p>

<h3>
  Large .dae files with BVH acceleration
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/max_part3.png" align="middle" width="400px"/>
        <figcaption>Rendering of maxplanck.dae (50801 primitives) with normal shading (took 0.103 s)</figcaption>
      </td>
      <td>
        <img src="images/lucy.png" align="middle" width="400px"/>
        <figcaption>Rendering of CBlucy.dae (133796 primitives) with normal shading (took 0.272 s)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  BVH acceleration comparison
</h3>
<p>
    To test our BVH algorithm, we compared the rendering times of moderately complex scenes with and without BVH accerlation. For cow.dae, with 5856 primitives, it took 3.6628s to render without BVH accerlation and 0.0339s to render with our BVH split huerisitc.  For teapot.dae, with 2464 primitives, it took 1.4207s to render without BVH accerlation and 0.0387s to render with our BVH split huerisitc.
</p>
<br>

<h2 align="middle">Part 3: Direct Illumination</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Direct Lighting: Uniform Hemisphere
</h3>
<p>
    To implement direct lighting hemisphere, we take the average of light on a point from num_samples random samples using Monte Carlo estimator. To calculate the light output from each sample, we first create a ray from the point of interest to a random sample from hemisphereSampler in the world frame. If this new ray intersects a light source, we calculate the light with the product of the light source emission from the intersection, bsdf and the cosine of the sampled ray. Then we divide by the probability density function, which is 1/(2*pi) for each sample. After we finish taking all the samples, we divide by num_samples to give us the  average light.
</p>

<h3>
  Direct Lighting: Importance
</h3>
<p>
    For direct lighting importance, we take the sum of light emitted from each light at the scene. To do this we iterate through all the lights at the scene and get their emitted radiance, pdf, distance to light and sample vector. We can use these values to create a ray from the point of interest to the light source. Then we check for no intersection for the ray because this means that there is no object between the hit poit and the light source. We also check if the light is behind the surface at the hit point by checking if the sample angle is not negative. If all these conditions are met, then we calculate the light with the product of emitted radiance, bsdf, angle of sample, and divide by pdf. If the sample is not a point light source, we will then sample the same light again for ns_area_light times and take the average from sampling the same light.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/bunny_hemisphere.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/bunny_importance.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres_importance_l=1.png" align="middle" width="200px"/>
        <figcaption>1 Light Ray (CBSpheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_importance_l=4.png" align="middle" width="200px"/>
        <figcaption>4 Light Rays (CBSpheres.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres_importance_l=16.png" align="middle" width="200px"/>
        <figcaption>16 Light Rays (CBSpheres.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_importance_l=64.png" align="middle" width="200px"/>
        <figcaption>64 Light Rays (CBSpheres.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
	In the above images, there is less noise seen in the soft shadows as the number of light rays increases.
</p>
<br>

<h3>
  uniform hemisphere sampling vs lighting sampling
</h3>
<p>
	Importance lighting gives a noticeable improvement over hemisphere lighting. This can be seen in the amount of noise that importance has compared to hemisphere lighting. In the images using importance, there is significantly less noise compared to images using hemisphere lighting in both the light that is cast on the objects as well as the shadows cast by the objects. In the two images of the bunny, hemisphere lighting causes the light source on the ceiling to have blurry edges, which are not present in the image using importance lighting. In addition, some of the lit areas are a bit brighter in importance lighting compared to hemisphere lighting.

</p>
<br>


<h2 align="middle">Part 4: Global Illumination</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Indirect Lighting
</h3>
<p>
    Indirect Lighting comes from at_least_one_bounce_radiance. To implement at_least_one_bounce_radiance, we first get the light emitted from one_bounce_radiance. Next we check if the depth of the ray is greater than one. If it is then we can continue bouncing at a 65% chance with Russian Roulette, or 100% chance if the depth of the ray is equal to the max ray depth. If these conditions are met then we sample the bsdf to get an emitted radiance, sample vector and pdf. We convert the sample vector into world frame and create a ray with one less depth. If the pdf and cos angle of the sample are greater than zero, we will check for an intersection. If there is one we will recursively call at_least_one_bounce_radiance with the new ray and new intersection and get the indirect light as a product of the recursive call result, emitted radiance, angle of sample. We will then divide by the pdf and normalize with the probability from Russian Roulette. We will add this to the light from one_bounce_radiance from earlier.
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_direct_indirect.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/CBspheres.png" align="middle" width="400px"/>
        <figcaption>example2.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
	These images below use different numbers of samples per pixel using 4 light rays. As shown, lower sample rates have a very high amount of noise, and as we increase the number of samples, the noise becomes less noticeable.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    YOUR EXPLANATION GOES HERE
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (example1.dae)</figcaption>
      </td>
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/your_file.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (example1.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    YOUR EXPLANATION GOES HERE
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Adaptive Sampling Implementation
</h3>
<p>
    Adaptive sampling tells us when the amount of light in a pixel has converged. We can check if the light has converged to end sampling early. This allows us to increase the number of samples to reduce noise in certain parts of the image without doing extra calculations for parts of the image that converge quicker. For each iteration of the sample, we calculate the current illuminance and squared illuminance of the sample and as well as the number of times we have sampled so far. Every samplesPerBatch iterations we use the accumulated values to calculate the mean and standard deviation to check if the convergance requirement has been met. The requirement is I <= maxTolarance * mean. We calculate the value I = 1.96 * (standard_deviation / sqrt(n)), where n is the number of times we have sampled so far. If it is satisfied then we can stop sampling.
</p>
<br>

<h3>
  Scenes rendered with 2048 samples per pixel and their sample rates.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/blob.png" align="middle" width="400px"/>
        <figcaption>Rendered image (blob.dae)</figcaption>
      </td>
      <td>
        <img src="images/blob_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (blob.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
